# HITL-LLM

few keywords: HITL, RLHF, aligning, human feedback, human preference, reasoning, LLM, Transformers

## Human Feedback
- [x] [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) 4 Mar 2022

Keyword match: fine-tuning with human-feedback, aligning language models with human intent

- [x] [Aligning Text-to-Image Models using Human Feedback](https://arxiv.org/abs/2302.12192) 23 Feb 2023

Keyword mathch: use the human-labeled image-text dataset to train a reward function that predicts human feedback, aligning such models using human feedback

- [ ] [HIVE: Harnessing Human Feedback for Instructional Visual Editing](https://arxiv.org/abs/2303.09618) 16 Mar 2023

Keyword match: harness human feedback for instructional visual editing, introduce scalable diffusion model fine-tuning methods that can incorporate human preferences based on the estimated reward

- [ ] [Fine-Grained Human Feedback Gives Better Rewards for Language Model Training](https://arxiv.org/abs/2306.01693) 2 Jun 2023

Keyword match: In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback

- [ ] [Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog](https://arxiv.org/abs/1907.00456) 30 Jun 2019

Keyword match: extract multiple different reward functions post-hoc from collected human interaction data, applying RL to real-world problems

- [ ] [Pretraining Language Models with Human Preferences](https://arxiv.org/abs/2302.08582) 16 Feb 2023

keyword match: pretraining with human feedback

-------Update at 2023.6.13-----------

- [ ] [Languages are Rewards: Chain of Hindsight Finetuning using Human Feedback](https://arxiv.org/pdf/2302.02676.pdf) 25 Mar 2023
- [ ] [Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision](https://arxiv.org/pdf/2305.03047.pdf) 4 May 2023
- [ ] [RRHF: Rank Responses to Align Language Models with Human Feedback without tears](https://arxiv.org/pdf/2304.05302.pdf) 22 May 2023
- [ ] [AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback](https://arxiv.org/pdf/2305.14387.pdf) 22 May 2023
- [ ] [Analyzing Influential Factors in Human Preference Judgments via GPT-4](https://arxiv.org/pdf/2305.14702.pdf) 24 May 2023
- [ ] [Human-in-the-Loop Interaction for continuously Improving Generative Model in Conversational Agent for Behavioral Intervention](https://dl.acm.org/doi/pdf/10.1145/3581754.3584142) 27 March 2023
- [ ] [ILLUME: Rationalizing Vision-Language Models through Human Interactions](https://arxiv.org/pdf/2208.08241.pdf) 31 May 2023
- [ ] [ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation](https://arxiv.org/pdf/2304.05977.pdf) 6 Jun 2023
- [ ] [Improving Multimodal Interactive Agents with Reinforcement Learning from Human Feedback](https://arxiv.org/pdf/2211.11602.pdf) 21 Nov 2022
- [ ] [Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback](https://arxiv.org/pdf/2305.14975.pdf) 24 May 2023
- [ ] [Nano: Nested Human-in-the-Loop Reward Learning for Few-shot Language Model Control](https://arxiv.org/pdf/2211.05750.pdf) 10 Nov 2022
- [ ] [Towards Boosting the Open-Domain Chatbot with Human Feedback](https://arxiv.org/pdf/2208.14165.pdf) 30 August 2022
- [ ] [Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor](https://arxiv.org/pdf/2212.09689.pdf) 19 December 2022
- [ ] [Fine-Tuning Language Models from Human Preferences](https://arxiv.org/pdf/1909.08593.pdf) 8 Jan 2023
- [ ] [WebGPT: Browser-assisted question-answering with human feedback](https://arxiv.org/pdf/2112.09332.pdf) 1 Jun 2022
- [ ] [Better Aligning Text-to-Image Models with Human Preference](https://arxiv.org/pdf/2303.14420.pdf) 25 Mar 2023

HITL related survey:
- [ ] [Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation](https://arxiv.org/pdf/2305.00955.pdf) 1 June 2023




