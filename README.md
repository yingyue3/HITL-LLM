# HITL-LLM

few keywords: HITL, RLHF, aligning, human feedback, human preference, reasoning, LLM, Transformers

## Human Feedback
- [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) 4 Mar 2022

Keyword match: fine-tuning with human-feedback, aligning language models with human intent

- [Aligning Text-to-Image Models using Human Feedback](https://arxiv.org/abs/2302.12192) 23 Feb 2023

Keyword mathch: use the human-labeled image-text dataset to train a reward function that predicts human feedback, aligning such models using human feedback

- [HIVE: Harnessing Human Feedback for Instructional Visual Editing](https://arxiv.org/abs/2303.09618) 16 Mar 2023

Keyword match: harness human feedback for instructional visual editing, introduce scalable diffusion model fine-tuning methods that can incorporate human preferences based on the estimated reward

- [Fine-Grained Human Feedback Gives Better Rewards for Language Model Training](https://arxiv.org/abs/2306.01693) 2 Jun 2023

Keyword match: In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback

- [Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog](https://arxiv.org/abs/1907.00456) 30 Jun 2019

Keyword match: extract multiple different reward functions post-hoc from collected human interaction data, applying RL to real-world problems

- [Pretraining Language Models with Human Preferences](https://arxiv.org/abs/2302.08582) 16 Feb 2023

keyword match: pretraining with human feedback

-------Update at 2023.6.13-----------

- [Languages are Rewards: Chain of Hindsight Finetuning using Human Feedback] (https://arxiv.org/pdf/2302.02676.pdf) 25 Mar 2023




